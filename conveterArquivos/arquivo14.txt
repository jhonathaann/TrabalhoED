DEFENDING DEEP REGRESSION MODELS AGAINST
BACKDOOR ATTACKS
Lingyu Du1, Yupei Liu2, Jinyuan Jia2, Guohao Lan1
1Delft University of Technology, 2The Pennsylvania State University
ABSTRACT
Deep regression models are used in a wide variety of safety-critical applications,
but are vulnerable to backdoor attacks. Although many defenses have been pro-
posed for classification models, they are ineffective as they do not consider the
uniqueness of regression models. First, the outputs of regression models are con-
tinuous values instead of discretized labels. Thus, the potential infected target of a
backdoored regression model has infinite possibilities, which makes it impossible
to be determined by existing defenses. Second, the backdoor behavior of back-
doored deep regression models is triggered by the activation values of all the neu-
rons in the feature space, which makes it difficult to be detected and mitigated us-
ing existing defenses. To resolve these problems, we propose DRMGuard, the first
defense to identify if a deep regression model in the image domain is backdoored
or not. DRMGuard formulates the optimization problem for reverse engineer-
ing based on the unique output-space and feature-space characteristics of back-
doored deep regression models. We conduct extensive evaluations on two regres-
sion tasks and four datasets. The results show that DRMGuard can consistently
defend against various backdoor attacks. We also generalize four state-of-the-art
defenses designed for classifiers to regression models, and compare DRMGuard
with them. The results show that DRMGuard significantly outperforms all those
defenses.
1
INTRODUCTION
Regression techniques are widely used to solve tasks where the goal is to predict continuous values.
Unsurprisingly, similar to their classification counterparts, regression techniques have been revolu-
tionized with deep learning and have achieved the state-of-the-art result in many real-world applica-
tions. Examples such as gaze estimation (Zhang et al., 2017b; 2019), head pose estimation (Borghi
et al., 2017; Kuhnke & Ostermann, 2019), and facial landmark detection (Sun et al., 2013; Wu & Ji,
2019), among many others (Lathuili`ere et al., 2019). Unfortunately, deep regression models (DRM)
inherited the vulnerabilities of deep neural networks (Gu et al., 2019; Liu et al., 2018b; Nguyen
& Tran, 2021; 2020; Turner et al., 2019) and did not escape from the threat of backdoor attacks.
Existing work (Sun et al., 2022) shows that an attacker can inject a backdoor trigger into a DRM
such that it outputs an attacker-chosen target vector for any input stamped with an attacker-chosen
backdoor trigger, while its predictions for clean inputs are unaffected. Therefore, given the wide
adoption of DRM in many safety-critical applications such as driver attention monitoring (Abuel-
samid, 2020; Berman, 2020), navigation of autonomous vehicles (Zeisl et al., 2015), and pedestrian
attention monitoring (Raza et al., 2018; Schulz & Stiefelhagen, 2012), backdoor attacks raise severe
safety concerns about the trustworthiness and robustness of DRMs.
Existing solutions to defend deep classification model (DCM) against backdoor attacks can be di-
vided into data-level (Chen et al., 2018; Guo et al., 2023) and model-level defenses (Liu et al., 2019;
Wang et al., 2019; 2022b). Data-level defenses aim to detect backdoored training or testing data,
while model-level defenses aim to detect a potentially backdoored model and unlearn the backdoor
behaviors. As we will discuss in Section 2, our work focuses on model-level defenses, as they are
more realistic and do not assume the defender has access to the backdoored training or testing data.
However, existing model-level defenses (Wang et al., 2022b; 2019; Wu & Wang, 2021) are designed
for DCMs. Our experiments show that they are ineffective when generalized and applied to DRMs
1
arXiv:2411.04811v1  [cs.LG]  7 Nov 2024

that are considered in this work. There are two underlying causes. First, distinct from DCMs (Wang
et al., 2017) for which the output space is discretized into a few class labels, the output space of
DRMs is continuous. Thus, it is infeasible (if not impossible) to enumerate and analyze all the
potential target vectors using existing defenses designed for DCMs to determine the infected target
(Wang et al., 2019) or the compromised neurons (Liu et al., 2019). Second, different from DCMs
that adopt arg max to obtain the final output, DRMs do not need arg max. This makes the backdoor
behavior of the backdoored DRMs different from that of the backdoored DCMs. Specifically, for
a backdoored DCM, the backdoor behavior is often triggered by the activation values of several
neurons in the feature space (Wang et al., 2022b), whereas for a backdoored DRM, it is triggered by
the activation values of all the neurons, which makes it harder to be detected or mitigated.
Our work: In this paper, we propose DRMGuard, the first framework to detect backdoored DRMs
in the image domain. DRMGuard is applied to a DRM to reverse engineer a potential trigger func-
tion, based on which we make the decision on whether the model has been injected a backdoor or
not. A major challenge to reverse engineering of the potential trigger function in the regression
domain is that the output is defined in the continuous space. To address this challenge, we formu-
late the reverse engineering as an optimization problem, which is based on both output-space and
feature-space characteristics of the backdoored DRMs that are observed in this paper.
To demonstrate the effectiveness of DRMGuard, we consider two regression tasks, and conduct
extensive experiments on four datasets for state-of-the-art backdoor attacks.
Our experimental
results suggest that DRMGuard is consistently effective in defending both input-independent at-
tacks, e.g., BadNets (Gu et al., 2019), and input-aware attacks, e.g., Input-aware dynamic attack
(Nguyen & Tran, 2020). Furthermore, we adapt four state-of-the-art backdoor defenses, i.e., Neural
Cleanse (Wang et al., 2019), FeatureRE (Wang et al., 2022b), ANP (Wu & Wang, 2021), and Fine-
pruning (Liu et al., 2018a), designed for classifiers to regression models and compare DRMGuard
with them. The results demonstrate that DRMGuard outperforms all of them by a large margin.
2
BACKGROUND AND RELATED WORK
Backdoor Attacks: Many backdoor attacks (Chen et al., 2017; Gu et al., 2019; Liu et al., 2018b;
Phan et al., 2022; Wang et al., 2022a;c; Yao et al., 2019; Zhao et al., 2022) have been proposed for
deep neural networks. They showed that an attacker can inject a backdoor into a classifier and make
it output an attacker-chosen target class for any input embedded with an attacker-chosen backdoor
trigger. Depending on whether the attacker uses the same backdoor trigger for different testing
inputs, we categorize existing attacks into input-independent attacks (Chen et al., 2017; Gu et al.,
2019; Liu et al., 2018b; Turner et al., 2019; Yao et al., 2019) and input-aware attacks (Koffas et al.,
2022; Li et al., 2021b; Nguyen & Tran, 2021; 2020; Salem et al., 2022). For instance, Gu et al.
(2019) proposed an input-independent backdoor attack by using a fixed pattern, e.g., a white patch,
as the backdoor trigger. Recently, researchers proposed to use input-aware techniques, such as the
warping process (Nguyen & Tran, 2021) and generative models (Nguyen & Tran, 2020), to generate
dynamic triggers varying from input to input. When extending those attacks to DRMs (Sun et al.,
2022), an attacker can inject a backdoor and make the model output a fixed vector (called target
vector) for any testing input with the backdoor trigger. Lastly, backdoor attacks were also studied
for graph neural networks (Xi et al., 2021; Zhang et al., 2021) and natural language processing (Shen
et al., 2021). They are out of the scope of this paper as we focus on attacks in the image domain.
Existing Defenses: We categorize existing defenses against backdoor attacks into data-level de-
fenses (Doan et al., 2020; Gao et al., 2019; Ma et al., 2023) and model-level defenses (Liu et al.,
2022; 2019; Wu & Wang, 2021; Xiang et al., 2022; Zeng et al., 2022; Zheng et al., 2022). Data-level
defenses detect whether a training example or a testing input is backdoored or not. They usually have
two major limitations: 1) training data detection defenses (Chen et al., 2018) are not applicable for
a given model that is already backdoored; and 2) testing input detection defenses (Doan et al., 2020)
need to inspect each testing input at the running time and incur extra computation cost, and thus are
undesired for latency-critical applications, e.g., gaze estimation (Zhang et al., 2020). Therefore, we
focus on model-level defense in this work.
Model-level defenses detect whether a given model is backdoored or not, and state-of-the-art meth-
ods (Guan et al., 2022; Qiao et al., 2019; Wang et al., 2019; 2022b; Xiang et al., 2022) are based
2

on trigger reverse engineering. Specifically, they view each class as a potential target class and re-
verse engineer a backdoor trigger for it. Given the reverse-engineered backdoor triggers, they use
statistical techniques to determine whether the classification model is backdoored or not. However,
existing solutions are mainly designed for classification tasks that have categorical output. As we
will show in this paper, they cannot be applied to DRMs. We note that Li et al. (2021a) also studied
backdoor defense for DRMs, but they only considered a specific attack designed in their paper where
the inputs of the regression model are low-dimensional vectors, i.e., five dimensional vectors. By
contrast, we consider DRMs in the high-dimensional image domain.
3
DESIGN OF DRMGUARD
3.1
THREAT MODEL
Deep regression model:
A deep regression model (DRM) is a deep neural network that maps
an input to a vector, i.e., f : X 7‚ÜíY, where X ‚äÇRNw√óNh√óNc represents the input space with
width Nw, height Nh, and channel Nc; and Y ‚ààRd represents the d-dimensional output space.
Given a training dataset Dtr that contains a set of training examples, we define the following loss
1
|Dtr|
P
(x,y)‚ààDtr ‚Ñì(f(x), y), where (x, y) is a training example in Dtr and ‚Ñìis the loss function for
the regression task (e.g., ‚Ñì2 loss) to update the parameters of f.
Backdoor attacks:
We consider existing backdoor attacks for classification models (Gu et al.,
2019; Nguyen & Tran, 2021; 2020; Turner et al., 2019), and adapt them to DRMs. Specifically,
given a training dataset Dtr, an attacker can add backdoor triggers to the training samples in Dtr,
and change their ground-truth annotations to an attacker-chosen vector, yT ‚ààY, known as the target
vector. The attacker can manipulate the training process. The backdoored DRM performs well on
benign inputs, but outputs the target vector yT when the backdoor trigger is present in the input.
Formally, we define the backdoor attack for DRMs as:
f(x) = y, f(A(x)) = yT ,
(1)
where f is the backdoored DRM, x ‚ààX is the benign input; y ‚ààY is the benign ground-truth
annotation; and A is the trigger function that constructs the poisoned input from the benign input.
Evaluation metric for backdoor attacks: Given a set of poisoned inputs, we define attack error
(AE) as the average regression error calculated from the output vectors and the target vector over all
the poisoned inputs, to evaluate the performance of backdoor attacks on DRMs. AE can be regarded
as the counterpart to the attack success rate for backdoor attacks on classification models.
Assumptions and goals of the defender:
The defense goal is to identify if a DRM has been
backdoored or not. Following existing defenses for backdoor attacks (Liu et al., 2019; Wang et al.,
2019; 2022b), we assume the defender can access the trained DRM and a small benign dataset Dbe
with correct annotations.
3.2
OVERVIEW OF DRMGUARD
Momentum 
reverse trigger
ùêπùêπ
ùë•ùë•
ùê∫ùê∫ùúÉùúÉ
ùêªùêª
Output space
Feature space constraints
Optimization objective based on output space characteristic
ùëìùëì
Figure 1: Overview of DRMGuard.
We propose DRMGuard to identify if a DRM has
been backdoored by reverse engineering the trig-
ger function A. Figure 1 shows the overview of
DRMGuard. We use a generative model GŒ∏ to
model A.
This allows us to model the trigger
function for both input-independent and input-
aware attacks. For a given DRM f under exam-
ination, we split it into two submodels. Specifi-
cally, we first use the submodel F to map the original input x to the feature space F(x) ‚ààRm,
i.e., the output space of the last but second layer of f. Then, we use the submodel H to map the
intermediate feature from the feature space to the final output space. This allows us to investigate
the characteristics of the backdoored DRMs in both feature space and output space, based on which
we formulate the optimization problem for reverse engineering. Moreover, we propose a strategy
called momentum reverse trigger to reverse high-quality triggers.
3

3.3
OBSERVATIONS AND INTUITIONS FOR BACKDOORED DEEP REGRESSION MODEL
Reverse engineering is performed by solving an optimization problem with constraints that are de-
signed based on observations in the input (Wang et al., 2019) or the feature space (Wang et al.,
2022b). Existing work (Wang et al., 2022b) shows that by using feature-space constraints, one can
reverse both input-independent trigger (Gu et al., 2019) and input-aware trigger (Nguyen & Tran,
2021; 2020) for a backdoored deep classification model (DCM). Following this trend, we consider
feature-space constraints when designing the reverse engineering for DRM. Below, we first discuss
the difference in the feature space between backdoored DCM and backdoored DRM. Then, through
theoretical analysis and experiments, we introduce the key observation for backdoored DRM.
Difference between backdoored DCM and DRM: A key observation for backdoored DCMs is
that the backdoor behavior is represented by the activation values of several neurons in the feature
space (Liu et al., 2019; Wang et al., 2022b). Specifically, when a trigger is present in the input, the
activation values of the affected neurons will drop into a certain range, making the backdoored DCM
output the attacker-chosen target class regardless of the activation values of the other neurons. This
is because, after applying a series of operations to the feature vector, a backdoored DCM utilizes
arg max to obtain the final classification output. As long as the activation values of the affected
neurons can make the target class have the highest probability, the influence of the other neurons on
the final classification output will be eliminated by arg max. By contrast, the final regression output
of a backdoored DRM is obtained by applying linear transformation (or followed by an activation
function) to the feature vector without using arg max. Thus, the activation value of each neuron in
the feature space contributes to the final output. This difference inspires us to take all the neurons
into consideration when searching for the feature-space characteristics of backdoored DRMs, rather
than looking at a few specific neurons only.
Theoretical analysis and metrics:
We use {hi}N
i=1 and {hp
i }N
i=1 to denote the feature vectors
extracted from a set of N benign inputs {xi}N
i=1 and a set of poisoned inputs {A(xi)}N
i=1, respec-
tively, where hi = F(xi) ‚ààRm and hp
i = F(A(xi)) ‚ààRm. We use yi,j and yp
i,j to denote the jth
component of the output vector yi = H(hi) ‚ààRd and yp
i = H(hp
i ) ‚ààRd. yp
i,j is calculated by:
yp
i,j = ‚Ñ¶(wj ¬∑ hp
i + bj) = ‚Ñ¶(‚à•wj‚à•2‚à•hp
i ‚à•2 cos Œ±p
i,j + bj),
(2)
where ‚Ñ¶(¬∑) is the activation function; wj ‚ààRm and bj ‚ààR are the weights vector and the bias of H
for the jth component of the output vector, respectively; Œ±p
i,j is the angle between hp
i and wj. Based
on Equation 1, we have yp
1,j ‚âàyp
2,j ‚âà¬∑ ¬∑ ¬∑ ‚âàyp
N,j if f is backdoored, which means œÉ2({yp
i,j}N
i=1)
is a small positive value, where œÉ2(¬∑) is the variance function. As shown in Equation 2, the value of
œÉ2({yp
i,j}N
i=1) is influenced only by ‚à•hp
i ‚à•2 and Œ±p
i,j, as ‚à•wj‚à•2 and bj are constant for a given DRM.
Moreover, when f is backdoored, Pd
j=1 œÉ2({yp
i,j}N
i=1)/d is a small positive value and influenced
by ‚à•hp
i ‚à•2 and Œ±p
i , where Œ±p
i = {Œ±p
i,1, ..., Œ±p
i,d} ‚ààRd. We use Œ±i,j to denote the angle between hi
and wj, and define Œ±i as Œ±i = {Œ±i,1, ..., Œ±i,d} ‚ààRd.
To further investigate how ‚à•hp
i ‚à•2 and Œ±p
i influence Pd
j=1 œÉ2({yp
i,j}N
i=1)/d, we introduce the ratio
of norm variance (RNV) and the ratio of angle variance (RAV), as two feature-space metrics:
RNV = œÉ2({‚à•hp
i ‚à•2}N
i=1)/œÉ2({‚à•hi‚à•2}N
i=1) and RAV = 1
d
d
X
j=1
œÉ2({Œ±p
i,j}N
i=1)/œÉ2({Œ±i,j}N
i=1).
(3)
Specifically, RNV compares the dispersion of {‚à•hp
i ‚à•2}N
i=1 and {‚à•hi‚à•2}N
i=1, while RAV compares
the dispersion of {Œ±p
i }N
i=1 and {Œ±i}N
i=1. RNV ‚â™1 indicates that when triggers are present in the
inputs, the feature vectors extracted by F have similar norms. RAV ‚â™1 means that the variance of
angles between {hp
i }N
i=1 and wj are much smaller than that between {hi}N
i=1 and wj for j = 1, ..., d.
Observations: We use four backdoor attacks, i.e., BadNets (Gu et al., 2019), Input-aware dynamic
attack (IA) (Nguyen & Tran, 2020), WaNet (Nguyen & Tran, 2021), and Clean Label (Turner et al.,
2019), to train backdoored DRMs on MPIIFaceGaze dataset (Zhang et al., 2017a) and Biwi Kinect
dataset (Fanelli et al., 2013). Table 1 shows the RNV and the RAV of the backdoored DRMs that
are trained by different backdoor attacks on the two datasets. The key observation is that RAV is
significantly smaller than 0.1 in all the examined scenarios. To further explore this observation,
the scatter plots in Figure 2 visualize {Œ±p
i }N
i=1 and {Œ±i}N
i=1 in all the examined cases. We can see
4

BadNets
Clean Label
IA
WaNet
BadNets
Clean Label
IA
WaNet
(a) Backdoored DRMs trained on MPIIFaceGaze under different backdoor attacks (d = 2).
BadNets
Clean Label
IA
WaNet
BadNets
Clean Label
IA
WaNet
(b) Backdoored DRMs trained on Biwi Kinect under different backdoor attacks (d = 3).
Figure 2: The plots of {Œ±p
i }N
i=1 and {Œ±i}N
i=1 (in degree) for backdoored DRMs trained on (a) MPI-
IFaceGaze dataset and (b) Biwi Kinect dataset. The spread of the data points shows that the angles
of the poisoned inputs are highly concentrated, while the angles of the benign inputs are scattered.
that the angles of the poisoned inputs are highly concentrated, while the angles of the benign inputs
are scattered, meaning that œÉ2({Œ±p
i,j}N
i=1) ‚â™œÉ2({Œ±i,j}N
i=1) for j = 1, ¬∑ ¬∑ ¬∑ , d. We summarize this
observation in the feature space as follows.
Key observation for backdoored DRMs in the feature space: Consider a set of benign inputs
{xi}N
i=1 and a set of poisoned inputs {A(xi)}N
i=1. Let W = {w1, w2, ..., wd}T ‚ààRm√ód be the
weights matrix of H, where wj ‚ààRm. Then, we have the following observation:
œÉ2 
B(F(A(xi)), wj)
	N
i=1

‚â™œÉ2 
B(F(xi), wj)
	N
i=1

for j = 1, 2, ..., d,
(4)
where B(v1, v2) = arccos (v1 ¬∑ v2)/(‚à•v1‚à•2‚à•v2‚à•2); and A is the trigger function.
Table 1: The RNV and RAV for four attacks on
two datasets. In all the examined cases, RAV
is significantly smaller than 0.1.
Attack
MPIIFaceGaze
Biwi Kinect
RAV
RNV
RAV
RNV
BadNets
0.0433
1.4499
0.0002
0.0012
IA
0.0489
2.5714
0.0015
0.0046
Clean Label
0.0328
0.0428
0.0803
0.0341
WaNet
0.0311
0.8528
0.0003
0.0015
Lastly, Table 1 shows that RNV is close to or
greater than 0.1 for backdoored DRMs trained by
BadNets, IA, and WaNet on MPIIFaceGaze, while
for the other examined cases, RNV is significantly
smaller than 0.1.
This is because, as shown in
Figure 2, when RNV ‚â•0.1, each component of
Œ±p
i , i.e., Œ±p
i,j for j = 1, ¬∑ ¬∑ ¬∑ , d, is almost 90‚ó¶, for
i = 1, ¬∑ ¬∑ ¬∑ , N, meaning that cos Œ±p
i,j is almost zero.
Thus, yp
i,j is insensitive to the change of ‚à•hp
i ‚à•2,
which allows RNV to be similar to or even larger than 0.1 but still maintain a low œÉ2({yp
i,j}N
i=1).
3.4
METHODOLOGY
Reverse engineering for DRM: One major challenge for reverse engineering for DRM is that the
target vector yT is defined in the continuous output space. As a result, it is impossible to enumer-
ate and analyze all the potential target vectors using existing reverse engineering methods designed
for DCM, which treat each class as a potential target class and reverse the trigger for it (Wang
et al., 2019; 2022b). To resolve this challenge, we propose to reverse engineer A by minimizing:
Pd
j=1 œÉ2  {fj(GŒ∏(xi))}N
i=1

/d, where fj(GŒ∏(xi)) is the jth component of f(GŒ∏(xi)) ‚ààRd. This
is intuitive, as for a backdoored f the term 1
d
Pd
j=1 œÉ2  {yp
i,j}N
i=1

will be a small positive value.
Thus, we can search for the target vector in the continuous output space by learning a trigger func-
tion A, modeled by GŒ∏, that can mislead f to map different inputs to the target vector without
enumerating all the potential target vectors. Moreover, based on the key observation of backdoored
DRMs defined in Inequation 4, we introduce the feature-space regularization term in the optimiza-
5

tion. Formally, we define the optimization problem for the reverse engineering for DRM as:
Œ∏‚àó= min
Œ∏
Œª1
d
d
X
j=1
œÉ2 
{fj
 x‚Ä≤
i

}N
i=1

+ 1
N
N
X
i=1
‚à•x‚Ä≤
i ‚àíxi‚à•1 + Œª2rf,
(5)
where rf = 1
d
Pd
j=1 œÉ2({B(F(x‚Ä≤
i), wj)}N
i=1)

œÉ2({B(F(xi), wj)}N
i=1); x‚Ä≤
i = GŒ∏(xi); and Œª1 and
Œª2 are the weights for the first and third objectives, respectively. The first objective in the optimiza-
tion problem aims to reverse engineer the poisoned inputs {GŒ∏(xi)}N
i=1 that lead to the same output
vector, regardless of their actual contents. The second objective is the input-space regularization
term (Wang et al., 2019; 2022b) that ensures the transformed input GŒ∏(xi) is similar to the benign
input xi. The final objective is the feature-space regularization term, which can lead to a much lower
variance for {ÀÜŒ±p
i,j}N
i=1 than that for {ai,j}N
i=1, where ÀÜŒ±p
i,j is the angle between F(GŒ∏(xi)) and wj.
Momentum reverse trigger: Directly solving the optimization problem defined in Equation 5 can
result in a sub-optimal solution (illustrated in Appendix A.1): the algorithm focuses on the important
regions in the input image, to which the DRM pay more attention, and directly adds perturbations
to these regions to destroy the task-related features. To avoid such a trivial solution, we propose the
momentum reverse trigger to assign different weights to different regions to balance the attention
of the DRM on the image, such that the algorithm can pay attention to all the image pixels and
search for the trigger that are injected into both important and unimportant regions. Details about
the momentum reverse trigger are given in Appendix A.1. After introducing the momentum reverse
trigger into Equation 5, we use OPT -DRMGuard to denote the final optimization problem.
Backdoor identification: By solving the optimization problem OPT -DRMGuard, we can obtain
the perturbation ‚à•x‚Ä≤
i ‚àíxi‚à•that transforms input xi to the potential target vector. We observe that the
amount of perturbation required to transform the input to the potential target vector for a backdoored
DRM is significantly smaller than that for a benign DRM. Based on this observation, DRMGuard
introduces the metric I(f) = 1( 1
N
PN
i=1 ‚à•x‚Ä≤
i ‚àíxi‚à•1, œµ‚à•ÀÜx‚à•1), to identify if a given deep regression
model is backdoored or not, where ‚à•ÀÜx‚à•1 is the input image that has the maximum L1 norm in the
benign dataset Dbe; œµ is a constant; and 1 is the indicator function that returns 1 (backdoored DRM)
if 1
N
PN
i=1 ‚à•x‚Ä≤
i ‚àíxi‚à•1 < œµ‚à•ÀÜx‚à•1 and 0 (benign DRM) otherwise. We set œµ = 0.03.
Backdoor mitigation: Once a given DRM is identified as a backdoored DRM, DRMGuard uses the
reversed trigger function GŒ∏ and the available benign dataset Dbe to generate a reversed poisoned
dataset Drp with the original correct annotations. Then, DRMGuard fine-tunes the given backdoored
DRM by using Dbe and Drp to unlearn the backdoor behavior.
4
EVALUATION
4.1
EXPERIMENTAL SETUP
Regression tasks: We consider two regression tasks, i.e., gaze estimation and head pose estima-
tion. Gaze estimation tracks where the subject is looking at, and plays a key role in a series of
safety-critical applications, such as user authentication (Eberz et al., 2019; Katsini et al., 2020) and
driver distraction detection (Berman, 2020). Similarly, head pose estimation has also been used in
many safety-related applications, such as the driver assistance system (Jha & Busso, 2016; Murphy-
Chutorian et al., 2007) and pedestrian attention monitoring (Schulz & Stiefelhagen, 2012).
Datasets: We consider four benchmark datasets, i.e., MPIIFaceGaze (Zhang et al., 2019), Columbi-
aGaze (Smith et al., 2013), Biwi Kinect (Fanelli et al., 2013), and Pandora (Borghi et al., 2017). For
each dataset, we randomly select 80% and 10% of the images from the dataset to form the training
dataset Dtr and the benign dataset Dbe, respectively. We use the remainder as the testing set Dte to
evaluate the performance of backdoor mitigation. Details of datasets can be found in Appendix A.2.
Backdoor attacks: We consider four state-of-the-art backdoor attacks, including two input-
independent attacks, i.e., BadNets (Gu et al., 2019) and Clean Label (Turner et al., 2019),
and two input-aware attacks, i.e., Iuput-aware dynamic attack (IA) (Nguyen & Tran, 2020) and
WaNet (Nguyen & Tran, 2021). We detail how to adapt these backdoor attacks to DRMs and the
effectiveness of them on DRMs in Appendix A.3.
6

Table 2: Backdoor identification performance of
DRMGuard on MPIIFaceGaze for different at-
tacks. DRMGuard can defend various attacks.
Attack
TP
FP
FN
TN
Acc
BadNets
10
1
0
9
95%
IA
10
1
0
9
95%
Clean Label
10
1
0
9
95%
WaNet
10
1
0
9
95%
Table 3: Backdoor identification performance
of DRMGuard on different datasets for WaNet.
DRMGuard is effective on various datasets.
Dataset
TP
FP
FN
TN
Acc
MPIIFaceGaze
10
1
0
9
95%
ColumbiaGaze
8
4
2
6
70%
Biwi Kinect
10
0
0
10
100%
Pandora
7
0
3
10
85%
Compared defenses: For backdoor identification, we compare DRMGuard with two state-of-the-
art methods, i.e., Neural Cleanse (NC) (Wang et al., 2019) and FeatureRE (Wang et al., 2022b).
Since the output space of DRM is continuous, we generalize them from classification models to re-
gression models by taking the potential target vector yt as the optimization variable. We provide de-
tails of the generalizations in Appendix A.4. For backdoor mitigation, we compare DRMGuard with
two state-of-the-art methods, i.e., ANP (Wu & Wang, 2021) and Fine-pruning (Liu et al., 2018a).
Evaluation metrics: Following existing work (Wang et al., 2022b), we use the identification ac-
curacy as the performance metric. In detail, given a set of DRMs including benign and backdoored
DRMs, the identification accuracy is defined as the percentage of correctly classified DRMs over
all examined DRMs. We also report the number of True Positives (TP), i.e., correctly identified
backdoored DRMs, False Positives (FP), i.e., benign DRMs recognized as backdoored DRMs, False
Negatives (FN), i.e., backdoored DRMs identified as benign DRMs, and True Negatives (TN), i.e.,
correctly recognized benign DRMs. Moreover, we use ROC-AUC score to compare backdoor iden-
tification performance between DRMGuard, NC, and FeatureRE, after obtaining the average pertur-
bations on Dbe for benign DRMs and backdoored DRMs.
To evaluate the performance of DRMGuard on backdoor mitigation, we generate a poisoned dataset
PDte by applying trigger function to all the images in Dte. We define defending attack error (DAE)
as the average regression error calculated from the output vectors and the correct annotations over
all the images in PDte. Details of regression errors for the two examined regression tasks are given
in Appendix A.2. We use DAE and AE on PDte as the evaluation metrics for backdoor mitigation.
Defense settings: Unless otherwise mentioned, we set Œª1 = 20 and Œª2 = 800 for gaze estimation,
and set Œª1 = 10 and Œª2 = 100 for head pose estimation, given task difference. We use ResNet18
(He et al., 2016) (without the dense layer) to implement F, and a dense layer without activation
function to implement H. We consider gaze estimation task with MPIIFaceGaze dataset and the
state-of-the-art input-aware attack WaNet.
4.2
EVALUATION RESULTS ON BACKDOOR IDENTIFICATION
DRMGuard is effective for backdoor identification: We conduct three experiments to evaluate
the backdoor identification performance. First, we evaluate the performance of DRMGuard in iden-
tifying backdoored DRMs trained by different attacks. Specifically, for each of the four backdoor
attacks, i.e., BadNets, Clean Label, WaNet, and IA, we train ten benign DRMs and ten backdoored
DRMs on MPIIFaceGaze dataset. The results are shown in Table 2, which indicate that DRMGuard
can identify backdoored DRMs trained by both input-independent and input-aware attacks, at an
average accuracy of 95%. Moreover, we visualize the estimation of the target vector during the
training process and the reversed trigger in the Appendix A.5 and A.6, respectively.
Second, we examine the backdoor identification capability of DRMGuard on different regression
tasks and datasets, i.e., MPIIFaceGaze, ColumbiaGaze, Biwi Kinect, and Pandora. Specifically, we
train ten benign DRMs and ten backdoored DRMs using WaNet for each dataset. The results are
shown in Table 3. The average identification accuracy of DRMGuard on different datasets is 87.5%,
which demonstrates the effectiveness of DRMGuard on various regression tasks and datasets.
Finally, we consider the scenario where the DRM is backdoored by multiple trigger functions with
different target vectors. We report the attacking details and evaluation results in Appendix A.7. In
brief, the results show that our method is effective on identifying DRMs with multiple backdoors.
DRMGuard outperforms state-of-the-art defenses:
Table 4 shows the ROC-AUC scores of
DRMGuard, NC, and FeatureRE for four backdoor attacks. We also report the scores when ap-
7

Table 4: ROC-AUC scores of different methods on
MPIIFaceGaze for different attacks. DRMGuard
significantly outperforms NC and FeatureRE.
Attack
NC
FeatureRE
DRMGuard
BadNets
0.270
0.730
1.000
IA
0.300
0.560
1.000
WaNet
0.940
0.560
1.000
Clean Label
0.005
0.545
1.000
All attacks
0.379
0.599
1.000
Table 5: Performance of backdoor mitigation
for different attacks. DRMGuard can miti-
gate backdoor behaviors for various attacks.
Attack
Undefended
DRMGuard
AE
DAE
AE
DAE
BadNets
3.25
14.85
17.21
3.59
IA
3.19
14.40
15.69
3.50
Clean Label
0.72
15.43
16.42
2.51
WaNet
1.31
15.90
15.36
3.29
plying the four backdoor attacks simultaneously. As shown, the ROC-AUC score of DRMGuard
is 1.000 in all the examined cases, which is significantly higher than that of NC and FeatureRE.
Besides, we notice that FeatureRE fails to find a trigger function that enables the backdoored DRM
to map different inputs to similar output vectors, which confirms our analysis that the feature-space
characteristic for backdoored DCM (Wang et al., 2022b) does not hold for backdoored DRM.
4.3
EVALUATION RESULTS ON BACKDOOR MITIGATION
Table 6: Performance of different
methods on backdoor mitigation.
DRMGuard outperforms baselines.
Method
AE
DAE
Undefended
1.31
15.90
DRMGuard
15.36
3.29
Fine-tuning
13.96
4.32
Fine-pruning
6.68
16.82
ANP
4.92
13.13
We train backdoored DRMs using BadNets, Clean Label, IA,
and WaNet on MPIIFaceGaze. Table 5 shows AE and DAE
of the undefended and mitigated backdoored DRMs, which
indicate that DRMGuard can mitigate backdoor behaviors for
various attacks. Specifically, DRMGuard can significantly in-
crease AE and decrease DAE for all the attacks, which indi-
cates that the output vectors of DRMs are far away from the
target vector and close to the correct annotations after back-
door mitigation, even though triggers are injected in the inputs.
We compare DRMGuard with ANP and Fine-pruning on backdoor mitigation. We also consider a
baseline, i.e., Fine-tuning, which directly uses the benign dataset Dbe to fine tune the backdoored
DRM. We report AE and DAE for different methods after backdoor mitigation in Table 6. The AE
for DRMGuard is significantly larger than that for other methods, while the DAE for DRMGuard is
much smaller than that for other methods, which shows the superiority of DRMGuard on backdoor
mitigation. Moreover, Fine-pruning and ANP are built upon the feature-space characteristics of
backdoored DCM and perform terribly on backdoored DRMs. This also confirms our analysis that
the feature-space characteristics of backdoored DRMs are different with that of backdoored DCM.
4.4
ABLATION STUDIES
Impact of weights and the size of benign dataset:
To investigate the impact of Œª1 and Œª2 in
Equation 5 on the performance of backdoor identification, we vary Œª1 and Œª2 from 10 to 30 and
from 600 to 800, respectively. Moreover, we study the impact of the size of Dbe on the identification
performance by changing the ratio p of benign dataset to the original whole dataset from 5% to 15%.
We report the results in Table 7. We observe that the performance of DRMGuard is insensitive to Œª1,
as the identification accuracy is almost stable with different Œª1. However, DRMGuard is sensitive
to Œª2 and the identification accuracy increases with Œª2. This observation proves that the proposed
feature-space regularization term is important for the identification of backdoored DRMs. We also
observe that as p decreases from 15% to 5%, the identification accuracy and the number of TN
decrease, while the number of TP remains stable. This is because, compared to a large p, it is easier
to find a small amount of perturbation that can lead to the backdoor behavior on a small p for benign
models. However, the identification accuracy is still 90% even when p = 5%.
Impact of feature-space regularization term (FSRT): We remove FSRT from OPT -DRMGuard
and show the results in Table 8, which indicate that all the DRMs are classified as backdoored DRMs.
We further observe that without the FSRT, DRMGuard cannot find a trigger function that can map
different inputs to similar output vectors. As a result, DRMGuard solves the optimization problem
by focusing on minimizing the distance between the poisoned and benign images, and returns a
small amount of perturbations, which leads to the misclassification of backdoored DRMs.
8

Table 7: Ablation study on the impact of different value of Œª1, Œª2 and p.
Metric
Different Œª1
Different Œª2
Different p
10
20
30
600
800
1000
5%
10%
15%
TP
10
10
10
10
10
10
10
10
10
FP
0
1
1
5
1
0
2
1
0
FN
0
0
0
0
0
0
0
0
0
TN
10
9
9
5
9
10
8
9
10
Acc
100%
95%
95%
75%
95%
100%
90%
95%
100%
Table 8: Ablation study on FSRT and MTR.
Method
TP
FP
FN
TN
Acc
w/o FSRT
10
10
0
0
50%
w/o MTR
10
10
0
0
50%
Table 9: Evaluation results on adaptive attack.
Attack
AE
DAE
Acc
WaNet
1.51
15.99
95%
Adaptive
5.71
15.01
95%
Impact of momentum reverse trigger (MTR): We remove the MTR from OPT -DRMGuard
and report the identification results in Table 8. As shown, all the benign DRMs are classified as
backdoored DRMs. This is because, without the MTR, DRMGuard will find a small amount of
perturbation and add it to the eye regions to destroy gaze-related features and fool the benign models.
In this way, the poisoned images reversed from different images are transformed by f to similar
output vectors, and DRMGuard fails to correctly recognize benign DRMs.
4.5
ADAPTIVE ATTACKS
When the attacker has the full knowledge of DRMGuard, one potential adaptive attack that can
bypass our method is to force the left and the right terms in Inequation 4 to have similar values.
Based on this intuition, we design an adaptive attack that adds an additional loss term Ladp with a
weight Œªadp to the original loss function of the chosen backdoor attack. We define Ladp as:
Ladp =
1 ‚àí1
d
d
X
j=1
œÉ2 
B(F(A(xi)), wj)
	Np
i=1

œÉ2 
B(F((xi), wj)
	Nb
i=1
,
(6)
where Np and Nb are the numbers of poisoned inputs and benign inputs in a minibatch. The loss term
Ladp tries to break the feature-space observation by enforcing RAV to be close to one. We generate
ten backdoored DRMs by the adaptive attack with Œªadp = 0.02. Table 9 shows the identification
accuracy and the averaged AE and DAE over ten backdoored DRMs. The AE of the adaptive attack
is significantly higher than that of WaNet. This proves that our feature-space observation of the
backdoored DRM is the key characteristic leading to the backdoor behavior. The adaptive attack
cannot reduce the identification accuracy of our method.
5
DISCUSSION AND LIMITATION
Discussion. To further investigate the performance of DRMGuard, we evaluate DRMGuard by
considering more backdoor attacks, different architectures of DRMs, and a larger set of DRMs
in Appendix A.8. The results show DRMGuard can consistently defend against various backdoor
attacks and can be generalized to different architectures. Also, DRMGuard maintains a similar
identification accuracy on a larger set of DRMs that contains more backdoored and benign DRMs.
Limitation. Similar to backdoor defenses (Wang et al., 2019; 2022b) for DCM, our method requires
a small benign dataset to identify backdoored DRM and mitigate backdoor behaviors.
6
CONCLUSION
We propose the first backdoor identification method DRMGuard for deep regression models in the
image domain. Our method fills in the gap where existing backdoor identification methods only
focus on deep classification models. Our comprehensive evaluation shows that our method can
defend against both input-independent and input-aware backdoor attacks on various datasets.
9

REFERENCES
Sam
Abuelsamid.
Seeing
machines
and
BMW
team
up
on
driver
monitoring
at
CSE.
https://www.forbes.com/sites/samabuelsamid/2020/01/07/
seeing-machines-and-bmw-team-up-on-driver-monitoring-at-ces/,
2020.
M. Barni, K. Kallas, and B. Tondi. A new backdoor attack in cnns by training set corruption without
label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), 2019.
Bradley Berman. Driver-monitoring systems to be as common as seat belts. https://www.sae.
org/news/2020/02/smart-eye-safety-driver-monitoring, 2020.
Guido Borghi, Marco Venturelli, Roberto Vezzani, and Rita Cucchiara. Poseidon: Face-from-depth
for driver pose estimation.
In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung
Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by
activation clustering. arXiv preprint arXiv:1811.03728, 2018.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
Bao Gia Doan, Ehsan Abbasnejad, and Damith C Ranasinghe. Februus: Input purification defense
against trojan attacks on deep neural network systems. In Proceedings of the Annual Computer
Security Applications Conference (ACSAC), 2020.
Simon Eberz, Giulio Lovisotto, Kasper B Rasmussen, Vincent Lenders, and Ivan Martinovic. 28
blinks later: Tackling practical challenges of eye movement biometrics. In Proceedings of the
ACM SIGSAC Conference on Computer and Communications Security (CCS), 2019.
Gabriele Fanelli, Matthias Dantone, Juergen Gall, Andrea Fossati, and Luc Van Gool. Random
forests for real time 3d face analysis. International Journal of Computer Vision, 101(3):437‚Äì458,
February 2013.
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal.
Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the Annual
Computer Security Applications Conference (ACSAC), 2019.
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access, 7:47230‚Äì47244, 2019. doi: 10.1109/ACCESS.
2019.2909068.
Jiyang Guan, Zhuozhuo Tu, Ran He, and Dacheng Tao. Few-shot backdoor defense using shap-
ley estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2022.
Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An
efficient black-box input-level backdoor detection via analyzing scaled prediction consistency.
arXiv preprint arXiv:2302.03251, 2023.
Aryaman Gupta, Kalpit Thakkar, Vineet Gandhi, and PJ Narayanan. Nose, eyes and ears: Head pose
estimation by locating facial keypoints. In Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), June 2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
10

Sumit Jha and Carlos Busso. Analyzing the relationship between head pose and gaze to model driver
visual attention. In Proceedings of IEEE International Conference on Intelligent Transportation
Systems (ITSC), 2016.
Christina Katsini, Yasmeen Abdrabou, George E Raptis, Mohamed Khamis, and Florian Alt. The
role of eye gaze in security and privacy applications: Survey and future HCI research directions.
In Proceedings of the ACM Conference on Human Factors in Computing Systems (CHI), 2020.
Stefanos Koffas, Stjepan Picek, and Mauro Conti. Dynamic backdoors with global average pooling.
In Proceedings of IEEE International Conference on Artificial Intelligence Circuits and Systems
(AICAS), 2022.
Felix Kuhnke and Jorn Ostermann. Deep head pose estimation using synthetic images and par-
tial adversarial domain adaption for continuous label spaces. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV), pp. 10164‚Äì10173, 2019.
St¬¥ephane Lathuili`ere, Pablo Mesejo, Xavier Alameda-Pineda, and Radu Horaud. A comprehensive
analysis of deep regression. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI), 42(9):2065‚Äì2081, 2019.
Xi Li, George Kesidis, David J Miller, and Vladimir Lucic. Backdoor attack and defense for deep
regression. arXiv preprint arXiv:2109.02381, 2021a.
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor
attack with sample-specific triggers. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV), October 2021b.
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International symposium on research in attacks,
intrusions, and defenses, 2018a.
Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. Back-
door defense with machine unlearning. In Proceedings of the IEEE International Conference on
Computer Communications (INFOCOM), 2022.
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu
Zhang. Trojaning attack on neural networks. In Proceedings of Network and Distributed System
Security (NDSS) Symposium, 2018b.
Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs:
Scanning neural networks for back-doors by artificial brain stimulation. In Proceedings of the
2019 ACM SIGSAC Conference on Computer and Communications Security, 2019.
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor
attack on deep neural networks. In Proceedings of European Conference on Computer Vision
(ECCV), 2020.
Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, and Yang Xiang. The ‚Äúbeatrix‚Äù
resurrections: Robust backdoor detection via gram matrices. In Proceedings of Network and
Distributed System Security (NDSS) Symposium, 2023.
Erik Murphy-Chutorian, Anup Doshi, and Mohan Manubhai Trivedi. Head pose estimation for
driver assistance systems: A robust algorithm and experimental evaluation. In Proceedings of
IEEE Intelligent Transportation Systems Conference, pp. 709‚Äì714. IEEE, 2007.
Anh Nguyen and Anh Tran. Wanet‚Äìimperceptible warping-based backdoor attack. arXiv preprint
arXiv:2102.10369, 2021.
Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In Proceedings of Confer-
ence on Advances in Neural Information Processing Systems (NeurIPS), 2020.
Huy Phan, Yi Xie, Jian Liu, Yingying Chen, and Bo Yuan. Invisible and efficient backdoor at-
tacks for compressed deep neural networks. In Proceedings of IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 2022.
11

Ximing Qiao, Yukun Yang, and Hai Li. Defending neural backdoors via generative distribution mod-
eling. In Proceedings of the Conference on Advances in neural information processing systems
(NeurIPS), 2019.
Mudassar Raza, Zonghai Chen, Saeed-Ur Rehman, Peng Wang, and Peng Bao. Appearance based
pedestrians‚Äô head pose and body orientation estimation using deep learning. Neurocomputing,
272:647‚Äì659, 2018.
Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic backdoor attacks
against machine learning models. In Proceedings of IEEE European Symposium on Security and
Privacy (EuroS&P), 2022.
Andreas Schulz and Rainer Stiefelhagen. Video-based pedestrian head pose estimation for risk as-
sessment. In Proceedings of IEEE International Conference on Intelligent Transportation Systems
(ITSC), 2012.
Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei
Yin, and Ting Wang. Backdoor pre-trained models can transfer to all. In Proceedings of the ACM
SIGSAC Conference on Computer and Communications Security (CCS), 2021.
B.A. Smith, Q. Yin, S.K. Feiner, and S.K. Nayar. Gaze Locking: Passive Eye Contact Detection for
Human?Object Interaction. In Proceedings of the ACM Symposium on User Interface Software
and Technology (UIST), Oct 2013.
Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep convolutional network cascade for facial point
detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3476‚Äì3483, 2013.
Yuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian Lou, Zichuan Xu, Xing Di, Yu Cheng, and
Lichao Sun. Backdoor attacks on crowd counting. In Proceedings of the 30th ACM International
Conference on Multimedia, 2022.
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
Label-consistent backdoor attacks.
arXiv preprint arXiv:1912.02771, 2019.
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In Pro-
ceedings of the IEEE Symposium on Security and Privacy (SP), 2019.
Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang,
and Xiaoou Tang. Residual attention network for image classification. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Tong Wang, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, and Ting Wang. An invisible
black-box backdoor attack through frequency domain. In Proceedings of European Conference
on Computer Vision (ECCV), 2022a.
Zhenting Wang, Kai Mei, Hailun Ding, Juan Zhai, and Shiqing Ma.
Rethinking the reverse-
engineering of trojan triggers. In Proceedings of Conference on Advances in Neural Information
Processing Systems (NeurIPS), 2022b.
Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against
deep neural networks via image quantization and contrastive adversarial learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022c.
Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In
Proceedings of the Conference on Advances in neural information processing systems (NeurIPS),
2021.
Yue Wu and Qiang Ji. Facial landmark detection: A literature survey. International Journal of
Computer Vision (IJCV), 127:115‚Äì142, 2019.
Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. Graph backdoor. In Proceedings of the USENIX
Security Symposium, 2021.
12

Zhen Xiang, David J. Miller, and George Kesidis. Detection of backdoors in trained classifiers
without access to the training set. IEEE Transactions on Neural Networks and Learning Systems,
33(3):1177‚Äì1191, 2022. doi: 10.1109/TNNLS.2020.3041202.
Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural
networks. In Proceedings of the ACM SIGSAC Conference on Computer and Communications
Security (CCS), 2019.
Bernhard Zeisl, Torsten Sattler, and Marc Pollefeys. Camera pose voting for large-scale image-based
localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV),
December 2015.
Yi Zeng, Si Chen, Won Park, Z. Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of
backdoors via implicit hypergradient. In Proceedings of the International Conference on Learning
Representations (ICLR), 2022.
Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. It‚Äôs written all over your face:
Full-face appearance-based gaze estimation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) Workshops, July 2017a.
Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. MPIIGaze: Real-world dataset
and deep appearance-based gaze estimation. IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 41(1):162‚Äì175, 2017b.
Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Mpiigaze: Real-world dataset
and deep appearance-based gaze estimation. IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 2019.
Xucong Zhang, Seonwook Park, Thabo Beeler, Derek Bradley, Siyu Tang, and Otmar Hilliges. Eth-
xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation. In
Proceedings of the European Conference on Computer Vision (ECCV), 2020.
Zaixi Zhang, Jinyuan Jia, Binghui Wang, and NZ Gong. Backdoor attacks to graph neural networks.
In Proceedings of the ACM Symposium on Access Control Models and Technologies (SACMAT),
2021.
Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong, Dakui Wang, and Kaitai Liang. Defeat:
Deep hidden feature backdoor attacks by imperceptible perturbation and latent representation
constraints. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR), June 2022.
Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel
lipschitzness. In Proceedings of European Conference on Computer Vision (ECCV), 2022.
13

A
APPENDIX
Roadmap: We provide the details of the momentum reverse trigger in subsection A.1. The details
of two regression tasks and four datasets are discussed in subsection A.2. Subsection A.3 describes
the details of backdoor attacks and discusses how to generalize them to regression tasks. Then, we
show the details about how to generalize Neural Cleanse (Wang et al., 2019) and FeatureRE (Wang
et al., 2022b) to the deep regression model (DRM) in subsection A.4. Next, we visualize the estima-
tion of the target vector during the reverse engineering process and the reversed poisoned images in
subsection A.5 and subsection A.6, respectively. After that, we detail multi-target backdoor attack
for DRMs in subsection A.7. We evaluate DRMGuard on more backdoor attacks, different architec-
tures of DRMs, and a larger set of DRMs in Appendix A.8. We show that our idea of minimizing
the variance can be extended to classification domain for trigger reverse engineering in Appendix
A.9. Finally, we report the technical details of DRMGuard in subsection A.10.
Experiment environment: We conduct experiments with Python 3.7.13 and Tensorflow 2.9.0 on an
Ubuntu 20.04 machine with a NVIDIA A10 GPU.
A.1
DETAILS OF MOMENTUM REVERSE TRIGGER
Illustration of the sub-optimal solution: We illustrate the sub-optimal solution of the optimization
problem defined in Equation 5 in Figure 3. Specifically, we use BadNets (Gu et al., 2019) to train a
backdoored DRM on MPIIFaceGaze dataset for gaze estimation, where the trigger is a red square,
added at the right bottom corner of the input images, as shown in Figure 3(b). Then, we optimize
GŒ∏ by solving the optimization problem defined in Equation 5. We show the residual map between
the benign image and the reversed poisoned image in Figure 3(d). We can see that solving the
optimization problem 5 fails to reverse the trigger but adds perturbations to the eyes region that
contains the most important features for gaze estimation (Zhang et al., 2017a).
Technical details: The idea of momentum reverse trigger is to assign different weights to different
regions to balance the attention of the DRM on the image, such that the algorithm can pay attention
to all the pixels and search for the trigger that are injected into both important and unimportant
regions. To do this, we first generate an attention map T (xi) ‚ààRNw√óNh for each input xi based
on the gradient of f w.r.t. xi. In detail, for each pixel xi[a, b], we obtain the corresponding value
T (xi)[a, b] in the attention map by T (xi)[a, b] = PNc
c=1 |‚àÇf/‚àÇxi[a, b, c]|. Then, we re-scale the
attention map to [0, 1) by dividing each component of the attention map by a number that is larger
than the maximum value in the attention map. Then, instead of directly feeding GŒ∏(xi) to f, we use
the following image as the poisoned image and feed it to the DRM:
x‚Ä≤
i = GŒ∏(xi) ‚äô(1 ‚àíT (xi)) + xi ‚äôT (xi).
(7)
A.2
DETAILS OF REGRESSION TASKS AND DATASETS
A.2.1
DETAILS OF REGRESSION TASKS
We consider two regression tasks, i.e., gaze estimation and head pose estimation. Below, we intro-
duce the details of each regression task.
Gaze estimation: We consider 3D full-face gaze estimation, where the gaze estimation model aims
to estimate the 3D gaze direction from the facial image. The 3D gaze direction is represented by
a two-dimensional vector, denoting the yaw and pitch angles of the gaze direction. Following the
existing works on gaze estimation (Zhang et al., 2020; 2017a), we use angular error as the regression
error, which is defined as the angle between the estimated and the real gaze directions.
Head pose estimation: We consider a head pose estimation model that takes a monocular image
input and outputs a three-dimensional vector to denote the Eular angle (yaw, pitch, roll) of head pose
(Gupta et al., 2019). We use L1 loss as the regression error.
A.2.2
DETAILS OF DATASETS
We consider four benchmark datasets in our evaluation and introduce the details of these datasets in
the following. We also introduce the training details of benign DRMs on each dataset.
14

(a)
(b)
(c)
(d)
Figure 3: The visualization of the sub-optimal solution: (a) the benign image; (b) the poisoned
image; (c) the reversed poisoned image when directly solving the optimization problem defined in
Equation 5; and (d) the residual map between the benign and reversed poisoned images. Solving the
optimization problem 5 fails to reverse the trigger but adds perturbations to the image region that
contains the most important features for gaze estimation.
MPIIFaceGaze (Zhang et al., 2019) is a benchmark dataset for gaze estimation and is collected from
15 subjects in their daily life. Each subject contains 3,000 images under different backgrounds, illu-
mination conditions, and head poses. The image resolution is 224 √ó 224. We use Adam optimizer
with a learning rate of 0.0001 to train the model for 10,000 training steps.
ColumbiaGaze (Smith et al., 2013) is a gaze estimation dataset collected from 56 subjects. Each
subject has 105 facial images. We crop the facial patches from the original images and resize them
to 224 √ó 224. We use Adam optimizer with an initial learning rate of 0.001 to train the model for
10,000 training steps. The learning rate is multiplied by 0.1 at 5,000 and 9,000 training steps.
Biwi Kinect Dataset (Fanelli et al., 2013) is used for head pose estimation. It is collected from 24
subjects, and each subject has 400 to 900 images. Note that we use the cropped faces of Biwi dataset
(RGB images) released by (Borghi et al., 2017). We resize the image to 112 √ó 112. We use Adam
optimizer with a learning rate of 0.0001 to train the model for 5,000 training steps.
Pandora Dataset (Borghi et al., 2017) is a dataset for head pose estimation, which includes more
than 250k RGB and depth images. We use the pre-processed dataset, Cropped faces (RGB images),
released by the authors, where the facial images are cropped from the original images. The pre-
processed dataset has 100 subjects, and contains more than 120,000 images. We use Adam optimizer
with an initial learning rate of 0.0001 to train the model for 10,000 training steps. The learning rate
is multiplied by 0.1 at 5,000 training steps.
A.3
DETAILS OF BACKDOOR ATTACKS
In this subsection, we describe the details of the backdoor attacks used in our evaluation and how to
adapt them to DRMs. We also show the effectiveness of backdoor attacks on DRMs.
BadNets (Gu et al., 2019) uses a fixed pattern as the backdoor trigger, and the poisoned inputs are
generated by pasting the backdoor trigger on the inputs. In our experiments, we use a 20 √ó 20 red
patch located at the right-bottom corner as the backdoor trigger. The poisoning rate is 5%.
Clean Label (Turner et al., 2019) also uses a fixed pattern as the backdoor trigger. To be more
stealthy, the trigger is only applied to images that belong to the target class for classification tasks.
In our experiments, we use a 20 √ó 20 red patch located at the right-bottom corner as the backdoor
trigger. To generalize Clean Label to regression tasks, we apply the trigger to the inputs whose
annotations are ‚Äùclose‚Äù to the target vector. Formally, we consider the images whose annotations y
satisfy ‚à•y ‚àíyT ‚à•‚â§Œ¥ as the target group to be poisoned. We apply the PGD attack to half of the
images in the target group to generate adversarial samples. To improve the performance of backdoor
attacks, we apply the trigger to both the adversarial samples and another half of the images in the
target group. This is because if we only apply trigger to the adversarial samples, the trained DRM
will regard the perturbations generated by the PGD attack as the backdoor trigger and ignore the
attacker-defined trigger pattern. Annotations of poisoned images are changed to the target vector.
WaNet (Nguyen & Tran, 2021) generates stealth backdoor triggers through image warping tech-
niques. The trigger is inserted into the images by applying the elastic warping operation. Note that,
15

WaNet needs to modify the standard training process to train the backdoored DRM, while BadNets
and Clean Label follow the standard way to train the backdoored DRM. To improve the performance
of the attack, we set the warping strength to 1. We set the grid sizes for gaze estimation datasets and
head pose estimation datasets to 28 and 14, respectively.
Input-aware dynamic attack (IA) (Nguyen & Tran, 2020) generates dynamic backdoor triggers
by using a trainable trigger generator, which takes benign images as inputs and outputs triggers
varying from input to input. A trigger generated by an input image cannot be used on another one.
Similar to WaNet, IA also needs to modify the training process.
Table 10: Effectiveness of different backdoor attacks on DRMs.
Metric
BadNets
IA
Clean Label
WaNet
Benign
AE
3.25
3.19
0.72
1.31
n/a
RE
2.61
2.33
1.82
2.32
2.35
Effectiveness of backdoor attacks on DRMs: We use attack error (AE) to evaluate the effective-
ness of backdoor attacks on DRMs. Given a set of poisoned inputs, we define AE as the average
regression error calculated from the output vectors and the target vector over all the poisoned in-
puts. AE can be regarded as the counterpart to the attack success rate for backdoor attacks on deep
classification models. Besides, we define RE as the average regression error calculated from the
output vectors and the correct annotations over a benign dataset. For each attack, we apply the cor-
responding trigger function to each image in the benign dataset Dbe to generate a set of poisoned
images PDte. We report AE over PDte and RE over Dbe to show the effectiveness of the above
mentioned backdoor attacks on DRMs in Table 10. The experimental results show that the AE of
different backdoored DRMs are pretty low, which are almost similar to the RE of these backdoored
DRMs. Also, the RE of these backdoored DRMs are similar to that of the benign DRM. This proves
the effectiveness of these backdoor attacks on DRMs.
A.4
DETAILS OF BASELINE DEFENSES
We generalize two state-of-the-art defenses, i.e., Neural Cleanse (NC) (Wang et al., 2019) and Fea-
tureRE (Wang et al., 2022b), to regression models. Since the output of DRM is defined in the
continuous space, we generalize them from classification models to regression models by taking the
potential target vector yt as the optimization variable. Below, we introduce the formal definitions of
the optimization problems for these two defenses after generalization.
Generalization of NC: We generalize the optimization problem defined in NC to DRMs as:
m‚àó, ‚àÜ‚àó, y‚àó
t = min
m,‚àÜ,yt
1
N
N
X
i=1
‚Ñì(yt, f(A(xi, m, ‚àÜ)) + Œª ¬∑ |m|,
(8)
where Œª is the weight for the second objective; A(¬∑) is a function that applies a trigger represented
by m and ‚àÜto the benign image x; and ‚Ñì(¬∑) is the loss function for training the DRMs. Specifically,
‚Ñì(¬∑) is ‚Ñì1 loss for gaze estimation and head pose estimation. The detailed description of A(¬∑) can be
found in (Wang et al., 2019).
Generalization of FeatureRE: We generalize the optimization problem defined in FeatureRE to
DRMs as:
Œ∏‚àó, m‚àó, y‚àó
t = min
Œ∏,m,yt ‚Ñì(H((1 ‚àím) ‚äôa + m ‚äôt), yt) + Œª3
N
N
X
i=1
‚à•GŒ∏(xi) ‚àíxi‚à•,
where t = mean(m ‚äôF(GŒ∏({xi}N
i=1))), a = F({xi}N
i=1),
s.t. std(m ‚äôF(GŒ∏({xi}N
i=1))) ‚â§œÑ1, ‚à•m‚à•‚â§œÑ2,
(9)
where m is the feature-space mask; Œª3 is the weight for the second objective; œÑ1 and œÑ2 are the
thresholds for two constraints, respectively.
16

BadNets
Clean Label
IA
WaNet
training step
training step
training step
training step
Dimension 1
Dimension 2
Figure 4: Visualization of the estimation of the target vector (two-dimensional vector) for DRMs
backdoored by different attacks on MPIIFaceGaze dataset during the reverse engineering process.
The two rows correspond to the first and second dimensions of the output of the DRMs. The red
curves denote the estimation of the corresponding dimension of the target vector, while the blue
curves denote that of the real target vector. The red curves can converge to the neighbor of the blue
curves, which means that DRMGuard can estimate the target vector.
A.5
ESTIMATION OF THE TARGET VECTOR
DRMGuard is able to estimate the target vector. Given a set of reversed poisoned images, we use the
mean vector of the corresponding output vectors as the estimation of the target vector. We visualize
the estimation of the target vector during the reverse engineering process for different backdoor
attacks, i.e., BadNets, Clean Label, IA, and WaNet, on MPIIFaceGaze dataset in Figure 4. Note
that, since it is infeasible to train a backdoored DRM f that precisely outputs the attacker-chosen
target vector for poisoned images, we use the vector, Ex‚ààDbef(A(x)), as the real target vector,
where E(¬∑) is a function to obtain the mean vector for a given set of vectors. Figure 4 shows that
the output vectors of the reversed poisoned images can converge to the neighbor of the real target
vector.
A.6
VISUALIZATION OF REVERSED POISONED IMAGES
To investigate if DRMGuard can reverse engineer the poisoned images, we show the benign images,
the original poisoned images generated by different backdoor attacks, and the reversed poisoned
images in Figure 5. Specifically, we randomly sample six benign images from the benign dataset Dbe
and show them in Figure 5(a). We then show the original poisoned images generated by BadNets,
Clean Label, IA, and WaNet, on MPIIFaceGaze dataset and the corresponding reversed poisoned
images in Figure 5(b), Figure 5(c), Figure 5(d), and Figure 5(e), respectively. DRMGuard is able to
reverse engineer the poisoned images that are close to the original poisoned images.
A.7
MULTI-TARGET ATTACK
Table 11: Backdoor identifica-
tion performance of DRMGuard
for multi-target WaNet.
TP
FP
FN
TN
Acc
10
1
0
9
95%
We consider the scenario that the DRM is backdoored by multi-
ple trigger functions with different target vectors. We generalize
WaNet such that it can insert two warping-based triggers into a
DRM. Specifically, we generate two different warping functions,
and each of them corresponds to a unique target vector. During
the training process, we randomly select 15% images in the mini-
batch to apply the first warping function and change their annotations to the corresponding target
vector. We then select another 15% images in the minibatch to apply the second warping function
and change their annotations to the corresponding target vector. We do not modify other procedures
of WaNet. We call the generalized WaNet multi-target WaNet. We train ten backdoored DRMs by
using multi-target WaNet and ten benign DRMs on MPIIFaceGaze dataset. We report the perfor-
mance of backdoor identification of DRMGuard in this scenario in Table 11, which shows that our
method is effective on identifying DRMs with multiple backdoors.
17

Original
Reversed
(a) Benign images.
Benign
Original
Reversed
(b) BadNets.
Benign
Original
Reversed
Clean labels
(c) Clean Label.
Benign
Original
Reversed
IA
(d) IA.
Benign
Original
Reversed
WaNet
(e) WaNet.
Figure 5: Comparison between (a) the benign images, and the original poisoned images and the
corresponding reversed poisoned images for (b) BadNets, (c) Clean Label, (d) IA, and (e) WaNet.
The reversed poisoned images are close to the original poisoned images.
18

Table 12: Backdoor identification performance
of DRMGuard for different architectures.
Architecture
TP
FP
FN
TN
Acc
ResNet18
10
1
0
9
95%
ResNet34
10
0
0
10
100%
MobileNetV1
7
0
3
10
85%
Table 13: Backdoor mitigation performance of
DRMGuard for different architectures.
Architecture
Undefended
DRMGuard
AE
DAE
AE
DAE
ResNet18
1.31
15.90
15.36
3.29
ResNet34
2.82
15.97
15.50
2.40
MobileNetV1
0.42
16.04
15.88
3.70
Table 14: Backdoor identification performance
of DRMGuard on more attacks.
Attack
TP
FP
FN
TN
Acc
Blend
10
1
0
9
95%
SIG
10
1
0
9
95%
Table 15: Backdoor mitigation performance of
DRMGuard on more attacks.
Attacks
Undefended
DRMGuard
AE
DAE
AE
DAE
Blend
2.94
14.51
12.82
4.56
SIG
1.83
15.96
16.11
2.93
A.8
MORE EXPERIMENTS
Evaluation on more architectures: We study the generalization ability of DRMGuard on differ-
ent architectures of DRMs. Specifically, beyond ResNet18, we consider two different architectures,
i.e., ResNet34 (He et al., 2016) and MobileNetV1 (Howard et al., 2017). For each architecture, we
train ten backdoored DRMs by WaNet and ten benign DRMs on MPIIFaceGaze dataset. We report
backdoor identification performance and backdoor mitigation performance for different architec-
tures in Table 12 and Table 13, respectively. The results show that DRMGuard can be generalized
to different architectures of DRMs.
Evaluation on more attacks: We further evaluate the performance of DRMGuard on defending
Blend attack (Barni et al., 2019) and SIG (Liu et al., 2020). For each attack, we train ten backdoored
DRMs by WaNet and ten benign DRMs on MPIIFaceGaze dataset. We report backdoor identifica-
tion performance in Table 14. We also report backdoor mitigation performance in Table 15. The
experimental results show that DRMGuard can consistently defend against various attacks.
Table 16: Backdoor identification
performance of DRMGuard on a
larger set of DRMs.
TP
FP
FN
TN
Acc
29
5
1
25
90%
Evaluation on a larger set of DRMs: We evaluate the back-
door identification performance of DRMGuard on a larger set
of DRMs. Specifically, we train 30 backdoored DRMs by
WaNet and 30 benign DRMs on MPIIFaceGaze dataset. We
report backdoor identification performance in Table 16. The
results show DRMGuard can reach the identification accuracy
at 90% on a larger set of DRMs.
A.9
GENERALIZATION TO CLASSIFICATION DOMAIN
We show that the idea of minimizing the variance in the output space can be extended to DCMs. By
doing so, we can identify backdoored DCMs without enumerating all the labels.
We use {S(C(A(xi)))}N
i=1 to denote the probability vectors obtained by a backdoored DCM
C(¬∑) from a set of poisoned images {A(xi)}N
i=1, where S(¬∑) is the softmax function. Intuitively,
the poisoned images will lead to similar probability vectors for a backdoored DCM. Therefore,
1
d
Pd
j=1 œÉ2({Sj(C(A(xi)))}N
i=1) will be a small positive value, where Sj(C(A(xi))) is the jth
component of S(C(A(xi))). Based on this intuition, we propose the following optimization objec-
tive to generalize DRMGuard from the regression domain to the classification domain.
Œ∏‚àó= min
Œ∏
Œª1
d
d
X
j=1
œÉ2  {Sj(f (GŒ∏(xi)))}N
i=1

+ 1
N
N
X
i=1
‚à•GŒ∏(xi) ‚àíxi‚à•1
(10)
Table 17: Backdoor identification
performance for DCMs.
TP
FP
FN
TN
Acc
7
0
3
10
85%
The first objective in the optimization problem aims to reverse
engineer the poisoned images {GŒ∏(xi)}N
i=1 that lead to the
similar probability vectors, regardless of their actual contents.
The second optimization term ensures the transformed images
GŒ∏(xi) is similar to original image xi. We remove the feature-
19

Original
Reversed
Figure 6: Illustration of original poisoned images (the first row) and reversed poisoned image (the
second row) for backdoored DCMs,
space regularization term designed for DRMs, since backdoored DRMs and backdoored DCMs have
different feature-space characteristics.
We train ten backdoored DCMs by BadNets and ten benign DCMs on Cifar10. We report backdoor
identification results in the Table 17. We visualize the reversed poisoned image in Figure 6. The
results show that our idea of minimizing the variance in the output space can be extended to the
classification domain for backdoor identification without enumerating all the labels.
A.10
TECHNICAL DETAILS OF DRMGUARD
We use a simple generative model to implement GŒ∏, which is similar to the generative model used
in (Nguyen & Tran, 2020). Before performing the reverse engineering, we pre-train GŒ∏ on the
benign dataset for 5,000 training steps. During the reverse engineering, the batch size for gaze
estimation datasets is 50, while that for head pose estimation datasets is 100. We use Adam optimizer
to train GŒ∏ for 2,000 training steps, taking about 16 minutes. The learning rates for gaze estimation
and head pose estimation are 0.0015 and 0.0001, respectively.
20

